<h1 align="center">AtomGPT</h1>
<h4 align="center">
    <p>
        <b >English</b> |
        <a href="https://github.com/AtomEcho/AtomGPT/blob/main/README.md">‰∏≠Êñá</a>
    <p>
</h4>

This article is generated by AtomGPT based on the Chinese readme. üéâüéâüéâ(If there are any translation errors, please be understanding)

‚ÄãTo be able to train a Chinese model that is similar in capability to ChatGPT, we have opened the AtomGPT project. 

The AtomGPT project is based on LLaMA‚Äôs model architecture and starts from scratch training with the aim of showing the evolution process of the model capabilities during training as well as experiencing the learning process of the models. 

(Ps: If this project can provide you with any help, please click ‚ÄúLike‚≠êÔ∏è‚Äù.)

## Online experience platform
The platform of AtomGPT model growth is built to visualize the change in capability during training and make it easier for testing. 

An online test portal has been provided, click on the right upper corner to experience (Note that there may be a queue due to limited GPU resources).

Experience Address: [https://grow.atomecho.cn/](https://grow.atomecho.cn/)

<img src="./assets/demo.gif"></img>

## Recent Updates

### Dynamic

- June 1st, Children‚Äôs Day! We are starting to push our model into Model Hub.
- May 13th - The first test of the training was conducted for the first time.
- April 28th - It has been decided that we will train a Chinese big model with ChatGPT capabilities close to it.
### Pre-Trained Model Update

- June 1st - Opened out pre-trained step 8000 on the basis of the model.
### Chat Model Update

- June 1st - Released a single turn dialogue model based on the pre-trained step 8000 and LORA commands microadjustment.

## Training Detail
We are using Transformers-based LLaMA models with the configuration of 13B from Meta, which is a starting point for implementing AtomGPT. The training process was conducted on 10 machines with 8 Nvidia A100 GPUs and BF16 precision.

The pre-training process continues‚Ä¶

Data sources include the following aspects: 

1. The Chinese data
The Chinese data is the primary training data for pretraining, which comes from several sources.
- The original 100TB of data was collected by AtomEcho from the internet and includes high-quality Chinese text in various forms such as encyclopedias, books, blogs, news articles, notices, novels, and WeChat accounts. This part of the data has been cleaned up gradually and will be added to the model over time.
- Data from Wikipedia‚Äôs Chinese version
- 200GB of open source data called ‚ÄúWuDao.‚Äù"
- The Clue open-source Chinese pre-training data and high-quality Chinese long text data after cleansing
- Recent years, about 150 Chinese natural language processing multitasking competition datasets.
- [MNBVC](https://github.com/esbatmop/MNBVC) cleaned out a part of the data set.

2. Other language data (primarily in English)
- wiki_en
- openwebtext
- c4

3. Code data
To improve the code generation ability of models, we added open-source large amounts of ü§ó data sets
- codeparrot/github-code-clean
- codeparrot/apps
- huggingface-course/codeparrot-ds-train
- code_search_net
- Bigcode-the-stack-dedup

4. Continuously update
- Please let us know if you have high-quality training data sets that we can use. We are grateful for your support and cooperation

## Model download
Download all models below on ü§óModel Hub

### Pre-training model

AtomGPT pre-training model can be used with Transformers directly loaded. 4-bit compressed models require loading [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ/blob/main/README.md) for loading

Model Name |ü§ó Model Loading Name |Download Link
--|--|--
AtomGPT_8k|AtomEchoAI/AtomGPT_8k|[Model download](https://huggingface.co/AtomEchoAI/AtomGPT_8k)

### chat model
AtomGPT-chat model can be used with Transformers directly loaded. 4-bit compressed models require loading [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ/blob/main/README.md) for loading

Model Name |ü§ó Model Loading Name |Download Link
--|--|--
AtomGPT_8k_chat|AtomEchoAI/AtomGPT_8k_chat|[Model download](https://huggingface.co/AtomEchoAI/AtomGPT_8k_chat)
AtomGPT_8k_chat_4bit|AtomEchoAI/AtomGPT_8k_chat_4bit|[Model download](https://huggingface.co/AtomEchoAI/AtomGPT_8k_chat_4bit)
## Local Inference and Fast Deployment

### Requirements for hardware reasoning
Model | Memory Requirement for Hardware-based Inference
--|--
16 bit | At least 32GB of memory (e.g., V100 and A100)
8 bit | At least 24 GB of memory (e.g., 3090)
4 bit | At least 12 GB of memory (e.g., 3060)

### Gradio quickly builds a question-answer platform

Based on gradio, the interface of question and answer has been implemented with a streaming output
```
python example/atomgpt_chat.py --model_name_or_path AtomEchoAI/AtomGPT_checkpoint_8k_chat
```
The 4bit model is loaded as if it‚Äôs needed. --is_4bit

```
python example/atomgpt_chat.py --model_name_or_path AtomEchoAI/AtomGPT_8k_chat_4bit --is_4bit
```

### Docker deployment question-answering interface

Getting ready

### Transformers code example
#### 8bit version
```
from transformers import AutoTokenizer, AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained('AtomEchoAI/AtomGPT_checkpoint_8k_chat',device_map='auto',torch_dtype=torch.float16,load_in_8bit=True)
model =model.eval()
tokenizer = AutoTokenizer.from_pretrained('AtomEchoAI/AtomGPT_checkpoint_8k_chat',use_fast=False)
input_ids = tokenizer(['<s>Human: ‰ªãÁªç‰∏Ä‰∏ãÂåó‰∫¨\n</s><s>Assistant: '], return_tensors="pt",add_special_tokens=False).input_ids.to('cuda')        
generate_input = {
    "input_ids":input_ids,
    "max_new_tokens":512,
    "do_sample":True,
    "top_k":50,
    "top_p":0.95,
    "temperature":0.3,
    "repetition_penalty":1.3,
    "eos_token_id":tokenizer.eos_token_id,
    "bos_token_id":tokenizer.bos_token_id,
    "pad_token_id":tokenizer.pad_token_id
}
generate_ids  = model.generate(**generate_input)
text = tokenizer.decode(generate_ids[0])
print(text)
```
#### 4bit verison
```
from transformers import AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_quantized(args.model_name_or_path, device="cuda:0")
tokenizer = AutoTokenizer.from_pretrained('AtomEchoAI/AtomGPT_checkpoint_8k_chat',use_fast=False)
input_ids = tokenizer(['<s>Human: ‰ªãÁªç‰∏Ä‰∏ãÂåó‰∫¨\n</s><s>Assistant: '], return_tensors="pt",add_special_tokens=False).input_ids.to('cuda')        
generate_input = {
    "input_ids":input_ids,
    "max_new_tokens":512,
    "do_sample":True,
    "top_k":50,
    "top_p":0.95,
    "temperature":0.3,
    "repetition_penalty":1.3,
    "eos_token_id":tokenizer.eos_token_id,
    "bos_token_id":tokenizer.bos_token_id,
    "pad_token_id":tokenizer.pad_token_id
}
generate_ids  = model.generate(**generate_input)
text = tokenizer.decode(generate_ids[0])
print(text)
```

## Common Problem List
1. HugginFace downloads slowlyÔºü

¬∑ We provide a downloading location in China, which is being prepared.

## Limitations
Although the models on this project have some Chinese understanding and generation capabilities, they also have limitations such as:

- They may produce harmful content that does not meet human preferences or values
- Due to computing power and data issues, related model training is insufficient, so there are still improvements needed for Chinese understanding

## Thanks

In preparation

## Disclaimer
This project is based on the Apache 2.0 open source license. When using third-party code, please strictly follow the relevant open source agreements accordingly. The generated content of models may be affected by factors such as model calculation, randomness and quantization loss, etc., and this project does not guarantee its accuracy. Any output from the related resources and results are subject to legal liability for which the project shall bear no responsibility nor assume any obligation arising therefrom or in connection thereto.

## Feedback on problems
If you have any problems, please file an issue on GitHub. Or add the developer‚Äôs WeChat ID (zhangzheng-thu) to your contact list and send a message with details of your problem

Before filing issues, please check if FAQ can solve your problem first. It is also recommended that you review past issues for solutions to similar problems.

Please be polite when asking questions in order to create harmonious discussion communities


[![Star History Chart](https://api.star-history.com/svg?repos=AtomEcho/AtomGPT&type=Date)](https://star-history.com/#AtomEcho/AtomGPT&Date)